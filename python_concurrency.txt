Asyncio
=======

Allows to run tasks concurrently with a single thread.
Ideal for web-server like situation with multiple thread requirement and high IO.

Uses event loop that is a single thread where tasks are queued & monitored. Any task that needs to do IO is handed over to OS and some other task is worked on in event-loop while waiting for OS to return from IO (and continue from next instruction after). Repeat till there are no more tasks in queue.

Threads/Tasks are not pre-empted (unlike in threading module which are OS threads). Task determines when they (await) give up control to go back to FIFO queue for task processing. "async" indicates that following fn/code will await (with exception of async generator or "asynch with")
Easier to manage data in asyncio than in threads since pre-emption is ruled out.

Coroutine - a fn that can pause execution (e.g. when encountering IO operation) and resume when blocking operation is completed. This allows python to run other code while fn waits for IO.

"async" creates coroutine; async fn wait until promise has not happened
"await" pauses coroutine.
"asyncio.run" to execute a coroutine on an event-loop. This coroutine will call/await other coroutines
"asyncio.create_task" creates a task that schedules a coroutine on event loop to run asap in non-blocking manner. Need to await the task completion e.g. task_1 = asyncio.create_task(coroutine(arg1, arg2))
"task.cancel" a long running task e.g. task_1.cancel() will raise CancelledError
"task.done" to check if task is done. e.g. task_1.done() # True/False
"asyncio.wait_for(tasl, timeout) waits for a task to complete with a time out which raises TimeoutError
"asyncio.shield" to prevent cancellation of task from a timeout. wait_for will raise TimeoutError but task is still running so await it in exception handling.
"asyncio.wait" wait for awaitable objects iterable and timeout or return when ALL_COMPLETED|FIRST_COMPLETED|FIRST_EXCEPTION.  Returns tuple of (done, pending) awaitables.
    asyncio.wait(aws, *, timeout=None, return_when=ALL_COMPLETED)
"asyncio.gather" run multiple async operations and get results when completed. Schedule the awaitable in task queue. Returns result as futures & in same order as input awaitables. If return_exceptions=True then any exceptions are added to result otherwise default action is to propogate exception immediately on awaiting task.
    gather(*aws, return_exceptions=False) -> Future[tuple[()]]

"Future" class object that returns value later (in future instead of now).
    future = Future()
    future.done() # now False (True/False)
    future.set_result(123)
    future.done()  # True
    result = future.result()

Awaitables (__await__) : Coroutines, Futures & Task.


Adding "async" in front of a regular function definition makes it return a coroutine object when it's invoked i.e. it's not invoked right away. It needs to be in an event loop e.g. with asyncio.run() - which creates the event loop, runs the function and closes the loop.
    result = asyncio.run( foo(x) )      # foo coroutine can call other coroutines.

    In other coroutines,
    result = await bar(x)               # run bar(x), wait for it's code to be run & return result

functions have a "async" in front and code "await"s for control back.
await can only be inside coroutines.
Start tasks with asyncio.create_task() & wait for them to run in paralle with "await asyncio.wait()".
Same set of tasks can be gathered with "ayncio.gather()" - which can also take coroutines generated by our "async def fn()".
Py 3.11 introduces task group that use context manager with to ensure all tasks are awaited


### Asyncio
import time
import asyncio

async def task_fn(interval, count):
    print("Task started. Count=", count)
    # time.sleep(interval)      # not an async function
    await asyncio.sleep(interval)
    print("Task ended. Count=", count)

async def main():
    sleep_iterval = ['1.25','1.30','1.20']
    start = time.perf_counter()
    # for interval,count in enumerate(sleep_iterval):
    #     await task(interval, count)

    # tasks = [asyncio.create_task(task_fn(interval,count))
    #                             for interval,count in enumerate(sleep_iterval)]

    # done, pending = await asyncio.wait(tasks, timeout=1.25) 	# if timeout not specified then all tasks will be done
    # print("Done tasks:")
    # for tsk in done:
    #     result = tsk.result()		# any exceptions in task would be raised now
    #     print(result)
    # print("Pending tasks:")
    # for tsk in pending:
    #     print(tsk)      # Task has a "wait_for"
    # print(done, pending)

    # tasks = [asyncio.create_task(task_fn(interval,count))
    #                              for interval,count in enumerate(sleep_iterval)]
    # results = await asyncio.gather(*tasks)  # can take co-routines directly which asyncio.await can't do

    co_routines = [task_fn(interval,count) for interval,count in enumerate(sleep_iterval)]
    results = await asyncio.gather(*co_routines, return_exceptions=True)	# use try-catch here if not using return_exceptions=True
    print(results)

    end  = time.perf_counter()
    print("All done", end-start)

if __name__ == '__main__':
    asyncio.run(main())




ThreadPoolExecutor
# Beazley youtube - https://www.youtube.com/watch?v=5-qadlG7tWo

import time
def fn(x,y):
    time.sleep(1)
    return x+y

from concurrent.futures import ThreadPoolExecutor
pool = ThreadPoolExecutor(8)
future_obj = pool.submit(fn, 2, 3)
future_obj.result   # blocks until thread is done and gives result=5
                    # any exception will propogate out

# handle result with callsback if don't want to block waiting for result
def exit_handle_result(result):
    try:
        print(f"Got {result.result()}")
    except Exception as e:
        print(f"Failed {type(e)})
...
pool = ThreadPoolExecutor(8)
# trigger computations with call back at entry...callback will be the exit
future_obj = pool.submit(fn, 2, 3)
future_obj.add_done_callback(exit_handle_result)

# one function using __enter__/__exit__ with yield
@inlined_future
def do_func(x,y):
    result = yield pool.submit(fn, x, y)    # code before yield w/pool.submit is enter-code; rest is exit-code
    print(f"Got {result})

run_inline_future(do_func_)











Threading - alternative concurrent.futures.ThreadPoolExecutor + queue for data exchange between threads
        - second alternative asyncio that avoids multiple OS threads. Asyncio has coperative threads where async/await determine where each thread will swap out - while regular threading has threads being preempted by OS based on IO/CPU cycles taken.

    Due to GIL in Cpython, only one thread can run at a time. Python interpreter is single process and always has main thread.
    So threads are good for processes with lot of IO. For CPU heavy process, use multiprocessing.
    Running CPU intensive task in threads may cause worse performance as threads get swapped in/out vs. just running linearly.
    OS tries to run multiple threads on other cores - but GIL is locked by active thread so OS just wasted CPU cycles due to python GIL limitation. So multicore processor is detrimental to python multithreading (Python v3.1). In Python v3.2 onward, there is a global var to request current thread to drop GIL. Secondary thread waits for 0.005s (5ms) waiting for active thread to do IO and drop or at end of 5ms of conditional var wait, thread-1 suspends and releases GIL. At this point, the thread on top of priorityQ is started.
    Interpreter switches between threads (queued in a priorityQ) when active threads blocks for IO or when sys.getswitchinterval() 0.005 i.e. 5ms is hit.
    Shared data protected by mutex and uses condition to signal between threads.

    To manage data in thread:
        (a) use thread-safe data structure like queue
        (b) use threading.local() to define what variable/attributes are going to be common between threads
        (c) threading.Lock to use as mutex


    Module functions:
    ----------------
    active_count() - # of active threads (excludes not-started or completed threads)
    enumerate() - list all active threads including daemon threads. active_count() is based on this.
    thread_object = current_thread()
    excepthook - for uncaught exceptions from thread.run. Ignores SystemExit from thread (to indicate it's done) else printed on stderr.
    get_ident() - get current thread's thread-identifier; a non-zero value.
    get_native_id() - get OS thread id
    main_thread() -  return main thread i.e. from where python started
    set|get trace() - set trace on all threads (?unsure?)
    getprofile() - profile fn set
    TIMEOUT_MAX - max value allowed for timeout in acquire()/wait() meathods of Locl/RLock/Condition. Over it gives OverflowError.

    stack_size([size]) - get thread stack size(typicall 32KB - where thread stores it's local data + recursion). Can also be set to specific size so that threads created after the size change have updated stack size
    local() is where thread specific values/data is saved. e.g. mydata = threading.local; mydata.x = 1


    Thread object: an activity that runs in separate thread.
        Thread created by (1) overrideing thread.run or (2) passing callable to constructor (Class derived from threading can override only __init__ & run).
        Thread started by t.start()  -- which calls t.run() and now is_alive() is true; active_count is up.
        main thread waits for t.join() -- can't exit if thread is not a daemon so use join() to specify your wait point else main won't exit at end of code and wait for thread completion.
            Caution on resources used in daemon threads since they can die if main ends.

        t1 = Thread(target=print, args=(1,2,3,), kwards={'debug': True}, daemon=False, name=None)
            # name is typicall "Thread-N"; daemon state copied from creating thread if not supplied so daemons create daemons.
        t1.start()  # thread is now active and awaits CPU timeslot. Call just once else RuntimeError
            # a subclass can invoke .run() that starts the callable set in __init__
        t1.join(timeout=None)   # wait until t1 terminates and calling thread is blocked
            # if using timeout, use t1.is_alive() to check for timeout vs thread terminated.
        Others:
            tt1.name, t1.ident; t1.native_id; t1.daemon

    Lock object: for synchornization; not owned by a particular thread
        state set by:  acquire() to locked  and release() to unlocked
        when state=locked then acquire(blocking=True, timeout=-1) will block
        supports contextmanager so can be used like:
            with lock:
                # acquire
                do mutually exclusive action
                # release

        Lock methods:
            acquire(blocksing=True, timeout=-1) : # returns True if locked. Can return False when it times out
            release()       # no return value; can be called by any thread (not just the one that acquired it!). On release, other waiting threads can block it
            locked()        # True if lock is acquired

        RLock methods:

        Event: one thread signals while one/many other threads wait (similar to Lock)
            E.g. use 2 events to form a communication between 2 threads; or event to make threads exit if set.
            event = Event()  # clear state
            event.set()
            event.is_set()
            event.clear()
            event.wait(timeout=-1)

        Semaphore: control how many threads can access a shared resource.
            It's a lock + counter
            semaphore.acquire()  # check if counts>0 then count-- & give access/permit else wait
            semaphore.release()  # increase the count

queue.Queue:
    thread-safe data sharing between threads - a queue with required locking semantics
        from queue import Queue
        q = Queue(maxsize=-1)
        q.put(1), q.put(2), q.put(3)        # if size is specified on creation, then put blocks (unless called with block=False, timeout=N in which case it throws queue.Full exception)
        print(q.get())      # blocks until some item is available (if block=False then raises queue.Empty exception)
        q.task_done()       # for each get, there should be task_done by consumer to indicate this item can be removed from queue as consumer as processed it. For each put, there should be a get()
        # block thread until all queue is processed (get & task_done)
        q.join()

        q.qsize()
        q.empty()



ThreadPoolExecutor performs betters instead of mutually creating/starting/joining threads. Overall memory usage is less and faster 2X than manual approach.

    from concurrent.futures import ThreadPoolExecutor
    with ThreadPoolExecutor() as executor:
        future = executor.submit(fn, arg1, arg2)
        # typical to map dict(future, input_args) and use concurrent.futures.as_completed
        for fut in as_completed(future_dict):
            # future.result() future.exception()

        results = executor.map(fn, zip(arg1_list, arg2_list))



Multiprocess:
    Where task is CPU intensive. Move away from single CPU core limitation of GIL/python interpreter to multi-core processing with multi-processing.
    Threading is good for IO based tasked that wait for IO to complete.
    Processes also have separate memory space so can't corrupt variables. Any data passing is done through picking so don't pass large objects back/forth between the processes.
    Processes take longer to start-up than threads (as they are heavier)

    import multiprocessing
    processes = [ multiprocessing.Process(cpu_intensive_fn, args=[filename,] for file in filnames]
    for p in processes:
        p.start()           # now has p.pid, p.is_alive()
    for p in processes:
        p.join()            # see p.exitcode for error return code
        # data can be returned via IPC like pipe or Queue


ProcessPoolExecutor - helps manage resources automatically. Based on host's total CPU cores.
    from concurrent.futures import ProcessPoolExecutor
    future = executor.submit(target=fn, args=(1,2,3))       # future.result, future.exception
    # executor.map(fn, [args_list])
    # executor.shutdown()  ... or use with/context manager


