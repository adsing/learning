Kubernetes up & running book 2nd edition
=========================================

Need to know docker. 

K8s started in 2014, open-source infrastructure (container deployment/orchestration) for cloud deployment (any sized compute node). 
Distributes system, microservices that are scalable & reliable.

Benefits:
o Velocity :: feature release quickly (hourly) & reliably. 
  - Immutability - containers/k8s provide immutable infra through artifact. Example of mutable infra is apt-get (get binaries and 
            install on top of existing binaries) so infra is not a single artifact but collection of incremental updates over time.
            Immutable infra = new/complete/immutable image... with update replacing the image (rollack easy to older image).
  - Declarative config - eberything in K8s is a declarative config object that represents the desired state of system.
            Opposed to imperative config where state = series of instructions/actions. E.g. run A; run B; run C.
            Declarative config defines "state". E.g. replicas = 3.
            Store declarative config in source control is "infra as code"            
  - online self-healing systems - continously monitor your current state to ensure it matches desired/config state. E.g. ensure replicas=3
            Less time spent on operations and maintenance.
            "Operator" application runs a software like SQL DBW with wrapper code to detect health + healing.
o Scalability with "decoupled" architecture
  - Decoupling of components through well-defined APIs (implementer/consumer balance with low communication overhead) and
    service load balancer (demand/supply balance).
  - Easy scaling for applications & clusters
            Due to immutability+declarative nature, scale to more replicas (or auto-scale)
            For cluster, add new machine with same image or using declarative config
  - Scaling teams with micro-services: Ideal team size=2 pizza (6-8 people for knowledge sharing, decision making and sense of purpose)
            Each team builds one micro-service, combination of teams/microservices.
            K8s abtractions for building decoupled microservice architecture:
              = pods (group container of different images i.e. micro-services into single deployable unit)
              = service (load balancing, naming, discovery to isolate one service from another)
              = namespaces (isolation, access control)
              = ingress obejcts (combine multiple microsserices into single external API)
  - Separation of concerns for consistency & scaling
            Decoupling of OS/machine and app.
            Small teams required for maintaining apps/OS/machines
            KaaS - K8s-as-a-service on cloud for very small teams
o Abstracting your infra - Separates developer from machine (portability across machine/clouds)
o Efficiency 
  - Abtraction aids thought-process
  - Mchine used optimally (better utilization)
  - Cheap to create test/development environments with namespace (test/dev)


K8s cluster has a control pane & many worker nodes.

Worker nodes run pods i.e. groups of containers/microservices that belong to same application. 
Pod can consiste of 1 or more containers.
Wroker node consits of pods and: 
 - kubelet daemon that talks to Control Pane's API server (whic pods to run & ensure desired pod state is maintained)
 - kube-proxy that redirects network traffic to correct pod & provide load balancing for pods
 - container runtime (runs containers on worker nodes by polling registry, start/stop containers & container resource manager)

ControlPane has a REST API for user via UI/CLI to interest with and supports ReplicationController (run N instances of this pod) and MigrationController (that allows upgrade/rollback).
Control plane has 
 - etcd (distribute key-value DB to store clusters state), 
  - Scheduler (schedule pods on cluster depending on available resources on worker nodes),
  - ControlManager (running controllers that manage cluster state) e.g. ReplicationController, DeploymentController (rolling updates & rollbacks)
ControlPane talks to worker nodes via APIServer.

   
Creating and running containers:
================================
K8s platform for managing distributes applications. First, need to know how to put applications in container images.
Application = source code + library (perhaps shared e.g. libc) + language runtime. Potential issue with shared library (not installed on a host; or different version).
So image & infrastructure should be immutable.
Container - creation & register/upload of image; services/infra to run these. Container apps run inside an OS container
K8s supports Docker imagesand other OCI-compatible images (Open Contaner Initiative).
Docker - each layer add/removes/modifies preceding layers. Each config line adds a layer.
Containers :: (a) System contains like OS - full boot cycle with serive like ssh. (b) Application container - run a single program (preferred method).

Building with Docker: 
1. ".dockerignore" - do not copy these files into image.
2. Dockerfile is recipe for how to build the container image. 

Example:
FROM node:10          # Start from a Node.js 10 (LTS) image 1
WORKDIR /usr/src/app  # Specify the current working directory inside the image in which all commands will run 2
COPY package*.json ./ # Copy package files and install dependencies 3
RUN npm install
COPY . .               # Copy all of the app files into the image 4
CMD [ "npm", "start" ] # The default command to run when starting the container 5

# create simple-node Docker image
$ docker build -t simple-node .
#run it on locahost:3000
$ docker run --rm -p 3000:3000 simple-node
$ docker build -t simple-node .

Pitfalls of Docker file:
1. files removed by subsequent layer are still present in parent image and thus making the container image bulky.
2. changes made to in-between layers require all subsequent layers to be rebuilt. So try to make changes to last lines of Dockerfile.
3. Don't do compilation on docker image - leaves all the build/link tools in image. That is unless you can do multi-stage builds (build in one stage and then use the binary build image in deployment build).
4. Security - dont built images with passwords baked in.

Multi-state builds in docker produces multiple images; each is a stage from which artifacts from preceding stages can be copied to current stage.

# STAGE 1: Build
FROM golang:1.11-alpine AS build
RUN apk update && apk upgrade && apk add --no-cache git nodejs bash npm   # Install Node and NPM
RUN go get -u github.com/jteeuwen/go-bindata/...  # Get dependencies for Go part of build
RUN go get github.com/tools/godep
WORKDIR /go/src/github.com/kubernetes-up-and-running/kuard
COPY . .  # Copy all sources in
# This is a set of variables that the build script expects
ENV VERBOSE=0
ENV PKG=github.com/kubernetes-up-and-running/kuard
ENV ARCH=amd64
ENV VERSION=test
RUN build/build.sh   # Do the build. Script is part of incoming sources.

# STAGE 2: Deployment
FROM alpine
USER nobody:nobody
COPY --from=build /go/bin/kuard /kuard    #copy from build image
CMD [ "/kuard" ]

$ docker build -t kuard .
$ docker run --rm -p 8080:8080 kuard

Storing images in remote repository (like Artifactory) :
$ docker login  #authenticate   dockerhub.com - public repo
$ docker tag myapp dockerhub.com/myaccount/myapp:v2
$ docker push dockerhub.com/myaccount/myapp:v2

Docker runtime
$ docker run -d --name myapp --publish 8080:80  dockerhub.com/myaccount/myapp:v2  # -d as daemon in background
$ curl http://localhost:8080  # test

#stop and remove container
$ docker stop
$ docker rm myapp  # otherwise when new image is tagged with myapp its associted with new image while old image still exists.
$ docker images    # list images on computer
$ docker prune     # remove stopped images/untagged images/unused image layers

Limiting resources used by container by exposing underlying Linux cgroup.
$ docker run -d --name myapp --publish 8080:80  --memory 200m --memory_swap 1G --cpu-shares 1024 dockerhub.com/myaccount/myapp:v2

$ docker rmi <tag or image>  #remove/delete image


MicroK8s theory:
================
High Availability:
3 types of nodes in a HA setup:
    1. voter node: replicate DB & vote in leader election
    2. standby nodes: replicate DB but don't vote for leader election
    3. spare: don't replicate DB and don't vote for leader election
Status lists the voter & spare nodes and whether HA is achieved or not.
Master control pane runs on all nodes and can use "microk8s *" cmds on any node.
If leader node crashes, a new one is elected within 5 sec. Do it gracefull with "microk8s leave" & "microk8s remove-node <node>".
If a new node is made a voter node then you may have to install the add-ons 

Strict Confinement: microk8s comes with a strict setup that block file/network etc access.

Classic setup (default):
    - Calico as CNI Networking Interface (configure image registry, 
    - dqlite (instead of etcd) as data store

ClusterAPI (CAPI) is API for cluster creation/config/mgmt as YAML file.
    Abstracts away infra prvisioning, networking etc.
    Runs on management clusters (and not provisioned cluster where workloads run). 

Dual stack IPv4/IPv6:
    Default setup to use IPv4
    
Add ons: Dashboard/DNS/Cert-manager/helm/ingress/metrics server/prometheus/registry & many from community (fluentd, argoCD, istio, traefik)

MicroK8s v1.28 setup on localhost (ThinkPad):
=========================================
setup : 
    sudo apt install microk8s --classic
    sudo usermod -a -G microk8s $USER
    sudo mkdir -p ~/.kube
    sudo chown -f -R $USER ~/.kube
    # microk8s start
    microk8s status --wait-ready
        microk8s is running
        high-availability: no
          datastore master nodes: 127.0.0.1:19001   # master control pane, replicate DB and participate in leader election
          datastore standby nodes: none             # replicate DB but don't participate in leader election
                                                    # spare nodes don't replicate DB or participate in leader election
        addons:
          enabled:
            dashboard            # (core) The Kubernetes dashboard
            ha-cluster           # (core) Configure high availability on the current node
            helm                 # (core) Helm - the package manager for Kubernetes
            helm3                # (core) Helm 3 - the package manager for Kubernetes
            metrics-server       # (core) K8s Metrics Server for API access to service metrics


$ alias ukubectl="microk8s kubectl"
$ microk8s kubectl get all --all-namespaces
...

Deployment:
$ kubectl create deployment nginx --image nginx
# deployment/nginx created
$ kubectl expose deployment nginx --port 80 --target-port 80 --selector app=nginx --type=ClusterIP --name nginx
# service/nginx created

$ ukubectl get all          # we see pod, service, deployment & a replica-set
    NAME                       READY   STATUS             RESTARTS   AGE
    pod/ngix-979d6455c-nlkzv   0/1     ImagePullBackOff   0          4m8s

    NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
    service/kubernetes   ClusterIP   10.152.183.1     <none>        443/TCP   19d
    service/ngix         ClusterIP   10.152.183.163   <none>        80/TCP    2m14s

    NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
    deployment.apps/ngix   0/1     1            0           4m8s

    NAME                             DESIRED   CURRENT   READY   AGE
    replicaset.apps/ngix-979d6455c   1         1         0       4m8s
# replicaset is auto-created; pods have random suffix but same name as deployment; ngix is exposed on IP 10.152.183.163:80
# note: deployment & pods are not ready so curl is not working

$ curl 10.152.183.163       # didn't work - certificate error in logs
    # Get "https://172.18.204.90:10250/containerLogs/default/ngix-979d6455c-nlkzv/ngix?tailLines=5000&timestamps=true": tls: failed to verify certificate: x509: certificate is valid for 172.18.206.232, 172.17.0.1, not 172.18.204.90

$ microk8s reset    #shuts down all components & restarts clean
$ microk8s stop     # when not using; can always start


Dashboard:
$ microk8s enable dashboard     # enables
    If RBAC is not enabled access the dashboard using the token retrieved with:
        microk8s kubectl describe secret -n kube-system microk8s-dashboard-token    # token retrival
    Use this token in the https login UI of the kubernetes-dashboard service.

    In an RBAC enabled setup (microk8s enable RBAC) you need to create a user with restricted
    permissions as shown in:
    https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md

$ microk8s dashboard-proxy  # didn't work as certifcate is not authorized for my IP (172.18.204.90)

    Dashboard will be available at https://127.0.0.1:10443

    Use the following token to login: eyJhbGciOiJSUzI1NiIsImtpZCI6InAyRTVmUmRoQmtFeGhJb3lzUEhQMXVaYjNGRHFrVlRKRmYzVU5WalJiNHMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJtaWNyb2s4cy1kYXNoYm9hcmQtdG9rZW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjVkNDM3NjU2LWViODAtNDJhYi1iZjc1LTE3NGIzZmRlNmI5YSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpkZWZhdWx0In0.aA3f18MDKn3_tUbRraWzl3NZMmt0VlQiSaK1C8FDrxziyDDcowqX_aglVMMQiWcWyUVqujRh2jVKu2IRjaDmCj-2LbSbogjcMjsexOGcs9bw9X6SwWB_1MdsOt7_fvxt2a3raDOc5x1NVr0gKQvZUebWfGKsloSTa9g7l8LcwoiMjnch9qpFQ_og1_-MnDEWSYnOYab0upUOPNYs_jFTjM1Cix6LJYOF544w0Icwj-Wsx9FlYxGTsqejJbFhNMm4qRfXz7vGa5duhagxc7WTqo75hLxfCLqxcJHeZj8IB_7sg6gbugR0_abLJRw4yBSeXPBnFV835izVJH2LWX9_lQ

    error: error upgrading connection: error dialing backend: tls: failed to verify certificate: x509: certificate is valid for 172.18.206.232, 172.17.0.1, not 172.18.204.90
## edit and add IP to /var/snap/microk8s/current/certs/csr.conf.template .. didn't work
## sudo microk8s refresh-certs -e ca.crt  ... starts browser but certificate error exists (not erroring out atleast)


$ microk8s kubectl -n kube-system describe service kubernetes-dashboard | grep Endpoints
     10.1.19.168:8443
Open firefox to https://10.1.19.168:8443 and enter token



Helm:
======
 -  as a package manager (like apt) for K8s i.e. packaging collection of yaml of K8s and distributing them. Bundle of yaml files is called Helm Chart.
 - as a templating engine. App made of multiple microservices (where each app name/port/image is different else everything is same). Define common blue print for all microservices in an app and replace dynamic values with placeholders like {{.Values.container.name}} these values come from a "values.yaml" file or through CLI with --set flag. Good for CI/CD.
 -  
  
