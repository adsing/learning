Args: positional or keyword
	def fn(positional_only_args, /, positional_or_keyword, *, keyword_only):

	
Ternary:
	a if a < b else b


f-string (f"...{var}..."
   f"{var=}"	# var=10
   f"{var%2=}"	# var%2=0
   f"{var=:FORMAT_STRING}"	# format string is related to type. Invokes __format__ method of class/type
   	f"{now=:%y-%m-%d}"
   	f"{float_number=:.2f}"
   f"{var!a}"	# !a -> ascii (converts non-ascii e.g. unicode into \U)
   f"{var!r}"	# !r -> print repr equivalent
   f"{var!s}"	# !s -> print str (default) 

   
Size of:
   import sys
   sys.getsizeof([0,1,2,3])	# bytes used by this data struct - does not count recursive data structs, only 1 level deep. Custom code to do recusrive size computation. 
   				# Object has ref.count + ptr.to.its.type at least + 2ptr for garbage collector ... all of which getsizeof() captures.
   
   PyObject has:
   	ob_refcount  
   	ob_type
   PyVarObject has:
   	ob_size		# len - actual size
   PyListObject has:
   	ob_item		# pointer
   	allocated	# how many elem "could" fit in ob_item memory - max size (returned by getsizeof)
   2 Garbage collection pointers
   So a list has 7 meta-data elements. Size of empty list = 7 * 8 = 56 bytes


__slots__ can be used to save memory taken by objects + restrict from adding more member variable (obj.NewVar). 
The instance loses __dicts__.
The class retains __dicts__ and gains additional __slots__ member along with descriptors to get/set these slots members implemented in C code to access member based on memory location (instead of __dict__). 
	obj.slotX = 'some value' 	# calls Class.__dict__['slotX'].__set__(obj, 'some_value'
	print(obj.slotX)		# calls Class.__dict__['slotX'].__get__(obj, Class)


Profilers are of 2 types (a) Tracing which have higher initial setup but very accurate reporting (b) Sampling
cProfile is tracing type; scalene are sampler type which also supports (threads, multiprocessing, Python vs C time, GPU, mem.trend, mem.leaks, copy time between python/native library)
Profile with cProfile module:
   $ python3 -m cProfile mycode.py    # simple/basic
   $ python3 -m cProfile -o mycode.prof mycode.py && snakeviz mycode.prof

   import cProfile
   import pstats
   with cProfile.Profile() as profiler:
       my_function()
   stats = pstats.Stats(profiler)
   stats.sort_stats(pstats.SortKey.TIME).
   # stats.print_stats()		# alternatel, pip install snakeviz
   stats.dump_stats(filename='code_profile.prof')
   
   $ snakeviz code_profile.prof
   
Profiling with scalene:
   $ scalene [options] mycode.py   
   	options:
   	 --reduced-profile (only show where code runs for atleast 1%)
   	 --html --outfile profile.html  (line, fn level profile)
   	 --cpu-only	(not memory but does GPU also)
   	
   
   
Benchmarking:
    1. set PYTHONDONTWRITEBYTECODE=1		# don't cache bytecode between test runs
    2. python -m timeit -s "from mymodule import function" "function()"		# -s part not included in benchmark time
    3. 
    P

Faster code tips:
   - use itertools/collections e.g. count() instead of loops with var increment
   - speed vs memory trade off (generator vs list) esp if only first elelement of list is needed then next(gen)
   - numpy/numba
   - code optimization e.g. right data struct/algorithm
   	- e.g. {}, [], () faster than dict(), list(), tuple() fn calls. Tuple faster than list
   	- unique elements of list while retaining order dict.fromkeys(mylist) but elements need to be hashable
   	- DRY vs calling too many fn (overheads).
   	- look for loop-invariant stmt (can be done out of loop), use comprehensions, correct data struct(tuple, frozenset, str, bytes, list/bytearray, set, dict)
   
   
Data containers : Class/DataClass/Dict/NamedTuple
	class Pet(typing.NamedTuple):		# Pure classes faster than dataclass (50%) & namedtuple (25%)! but more code. Classes similar to dict.
	   legs:int
	   noise:str 

	@dataclass
	class Pet:
	   legs:int
	   noise:str

	Pet = namedtuple("Pet","legs noise")


Dataclasses: provides __init__, __repr__ and __eq__ by default. 

from dataclasses import dataclass, field, astuple, asdict
import inspect

@dataclass(frozen=True, order=True)			# order provides le/ge/lt/gt, frozen allows obj value to NOT be modified after initializations. 
							# Others: kw_args=True, match=True for structural pattern matching, slots=True
class Comment:
    id:int
    txt:str
    replies: list[str] = field(default_factory=list)	# list or any other fn that generates initial value. Factory is invoked if user doesn't specify this field while creating object. Can set "init=False" in field() to ensure that user can not provide this arg. "repr=False" to hide field from printing with repr. "compare=False" to skip the field 
    				# __post__init__(self) method can be used to set attributes that depend on other member attrib.
    	

def main():
    comment = Comment(1,"hoo")
    print(comment)
    # comment.id=2
    # comment.set_id(2)
    # print(comment)
    comment.x = 100	# can't do if frozen
    print(comment.x)
    number, comment = astuple(comment)		# dataclass converts to tuple/dict

    pprint(inspect.getmembers(Comment, inspect.isfunction))


if __name__ == '__main__':
    main()
    

match statement (for structural pattern matching) from Py3.10. 
Can match single value (int/str) or other type like list/dict and other objects
Match patterns based on given sequence i.e. order is important.
	match var:
		case 0: pass
		case 1|2: pass
		case 3 if condition: pass	#  with if condition
		case 3: pass
		case _: pass
		case other: print("unknown")

Decorator:
	Enhance/Modify behavior to existing fn. E.g. add time taken by existing fn; add logging/user behavior; authentication before fn calls.
	Address cross-cutting concerns (impacts multiple fn).
	Decorator takes a fn as input and returns modified fn as output.
	Can have multiple decorator on fn. E.g. @logging @timer on fn
	Caution - decorator impacts readability/understanding of code. e.g. when multiple decorator are used. Which one kicks in first e.g. logging before timing or after.
	
	def my_decorator(func, logger):		# instead of using functools.partial you can create an another outer layer of fn that takes logger as arg and returns my_decorator (see https://stackoverflow.com/questions/5929107/decorators-with-parameters)
	  @functools.wraps(func)		# fixes func.__name__ not to print "wrapper" but actual fn. Fixes fn.docstring and fn.name
	  def wrapper(*args, **kwargs):
	    logger.info(f"Started function {func.__name__}")
	    ret = func(*args, **kwargs)
	    logger.info("Ended")
	    return ret
	  return wrapper
	  
	@my_decorator(logging)		# inc = decorator(logging)(inc)   ... this is what it means and it won't work here because my_decorator needs 2 args. Use functools.partial or wrap my_decorator in another fn lets call it decorator_generator(logger)
	def inc(x):
	  return x+1

	my_decorator_with_default_logging = functools.partial(my_decortor, logger=logging)
	
	@my_decorator_with_default_logging
	def dec(x):
	  return x-1
	  
	inc(3)


Functools: Fn/Operations on callables (Fn, Class, Obj)
	@cache - memoize - store return values for args
	@cache_property - cache for methods; note: @property requires a setter fn but @cache_property allows writes
	@lru_cache(maxsize=N, typed=False) - args/kwargs must be hashable. Implemented with @cache_property but slightly slower as it needs to handle max_size limits.
		- has additional data like cache_info() to show hits/misses/currsize.
		- typically for expensive/recursive compute e.g. fibonacci or news web-page fetch
	@total_ordering - adds le/ge/lt/gt methods based on 2 methods that user needs to give i.e. eq and one of le/ge/lt/gt
	@partial - simplifies fn args/kwards by pre-building partial fn object that has some args/kwargs supplied in advance so that partial fn is called with remaining args/kwargs. Similarly, @partialmethod for methods/property.
	@reduce - apply fn to iterable with a default start value e.g. sum of series
	@wraps - calls update_wrapper that updates wrapper fn to look like wrapped function. Assigns __module__, __name__, __doc__, __annotations__, __qualname__. Updates __dict__. Used with decorators specially
	@singledispatch - transform fn into single-dispatch generic fn (based on type of first arg only). Declare the default fn with @singledispatch and then register other variants based on type(s) with @fn.register(type). To check which fn will be called for a given type, test with fn.dispatch(type) .. gives address of fn (not name!).
	@singledispatchmethod - like singledispatch on methods but applies on first non-self/non-cls arg.

Itertools: Creates iterators for efficient looping
	- non-terminating/infinite iterators
		- count(start, [step]]: count() from 0/start onwards
		- cycle(list_elements): cycle('0123456789')  ... 0123456789 0123456789 0123456789
		- repeat(element, [n)]: repeat('xyx', 20)  ...xyz xyz
	- terminating on shortest input sequence:
		- accumulate(list, [func=sum]): accumulate([1,2,3], lambda x,y: x+y) gives 1,3,6 (get running totals/products)
		- chain(list1, list2): chain any 2 iterables one after another
		- chain.from_iterable(iterable): chain iterable of iterables
		- compress(data_list, selector_list_bool): removes elements marked as false in selector list
		- dropwhile(predicate, sequence): chopping leading sequence. i.e. predicate applied to "leading" sequence; drops leading sequence if initially predicate is true and once false, ignores predicate and generates till end of sequence.
		- filterfalse(predicate, sequence): opposite of filter(which keeps elements where predicate is true), this keeps elements where predicate is false
		- groupby(iterable, [key]): sub-iterators groups by value of key(v)
		- isslice(sequence, [start, stop, step]): like slice/range for sequence
		- pariwise(iterable): ABCD gives overlapping pairs of AB BC CD
		- starmap(function, sequence): gives sequence of fn(*seq[0]), fn(*seq[1]). i.e. fn args come from sequence (as sub-lists) and returns sequence of results
		- takewhile(predicate, sequence): opposite of dropwhile, it gives the leading sequence while predicate is true
		- tee(iterator, n): splits one iterator in N-iterators. Allows reuse/copy of iterator.
		- zip_longest(seqA, seqB, [fillValue='-']) gives tuple having first element from seqA and second frmo seqB. Any missing values are replaced with None/fillValue.
	- Combination/Permutations
		- product(seqA, seqB, [repeat=1]) Cartesion product
		- permutations(seq, length) all combinations but no repeats e.g. AA but can have AB and BA
		- combinations(seq, length) permutations but can't have AB == BA
		- combinations_with_replacement(seq, length) allows AA

Operator: efficient operator fn
	- le, ge, lt, gt, eq, ne
	- not, is_, is_not, truth (or bool)
	- abs(x), add, sub, mul, floordiv, truediv, index (convert non-int to int for use as index), inv/invert, mod (%), pow, matmul (@), pos (postivie),  
		-- in-place operators (x+=y is x= operator.iadd(x,y)). Returns updated value if inputs are immutable else updates in place.
		-- iadd/iconcat (a+=b), a&=b, a<<=b, a//=b, a@=b,
	- lshift, rshift,     neg, or_, and_, xor
	- concat, contains(a,b ... b in a), countOf, indexOf, get/set/delitem 
	- call
	- attrgetter & itergetter:
		-- f = attrgetter('name.first', 'name.last'), then call f(b) returns (b.name.first, b.name.last).
		-- g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3]).
			getcount = itemgetter(1)
			sorted(inventory, key=getcount)
			# [('orange', 1), ('banana', 2), ('apple', 3), ('pear', 5)]

Collections: container datatypes
	- ChainMap: create a "view" to treat many dict/mappings as one. Internally store as references to dicts as a list (so update on any dict is reflected in ChainMap view). Can be updated/deleted but impacts only first dict/map in list.
				Can create new & separate "child" ChainMaps that append to existing ones.
				Can be used to select from options ChainMap(args, env, default).
					defaults = {'user': 'guest',...}
					parser = argparse.ArgumentParser()
					parser.add_argument('-u', '--user')
					namespace = parser.parse_args()
					command_line_args = {k: v for k, v in vars(namespace).items() if v is not None}
					combined = ChainMap(command_line_args, os.environ, defaults)
					combined['user']

		da = {'a':1,'b':2}
		db = {'a':3,'c':4}		# a:3 is not visible as a from da comes first
		dc = {'k':5}
		cm = ChainMap(da,db,dc)
		dc['x']=99		# visible in ChainMap
		cm['z']=100		# adds to da
		for key in cm:
			print(key, cm[key])

    - Counter : a dict subclass for counting hashable objects. Store {Key: Counts}. Any missing key gives count 0. 
				Methods: c.most_common(5), c.total(), c.subtract(another_counter)
				Addition and subtraction combine counters by adding or subtracting the counts of corresponding elements. 
				Intersection and union return the minimum and maximum of corresponding counts. 
				Equality and inclusion compare corresponding counts. 
				**Each operation can accept inputs with signed counts, but the output will exclude results with counts of zero or less.
				c = Counter(a=3, b=1, c=0)
				d = Counter(a=1, b=2, d=-1)
				c + d                       # add two counters together:  c[x] + d[x]
				# Counter({'a': 4, 'b': 3})
				c - d                       # subtract (keeping only positive counts)
				# Counter({'a': 2, 'd': 1})
				c & d                       # intersection:  min(c[x], d[x])
				# Counter({'a': 1, 'b': 1})
				c | d  
				Counter({'a': 3, 'b': 2})
				-d (or +d) removes zero/negative counts from result
				Counter({'d': 1})  (or Counter({'b': 2, 'a': 1}))

	- deque (double ended queue). Called "deck". 
			Optimized for stack(LIFO) and queue(FIFO) operations in O(1) 
			Can be unbounded or length given - in which case any new adds once size limit is reached causes overflow from other end.
			Recipe to use for moving average of 3 elements.
				dq = deque('abcdef', maxlen=5)	# gives bcdef
				dq.append('g')		# cdefg
				dq.appendleft('b')	# bcdef
				f = dq.pop()		# bcde
				b = dq.popleft()	# cde
				c,e = dq[0], dq[-1]
				dq.rotate(1)		# ecd 
				dq.rotate(-1)		# cde
				dq.extend('xyz')	# dexyz
				dq.extendleft('ABC') # CBAde <<note the order of insert
				dq.clear()

	- defaultdict(default_factory=None):
		Returns dict like object that has __missing__ method added (called by __getitem__) that invokes default_factory (if defined else KeyError) to setup a default value.
		Note: default_factory is not called for d.get() but only for d['item'] (__getitem__)
		Note: Py3.9 adds dict merge(|) and update(|=).
		
		Faster than an equivalent technique using dict.setdefault()
			for k, v in s:
    			d.setdefault(k, []).append(v)   # without defaultdict
		Example:
			exception_tracking = defaultdict(list)
			exception_tracking[exception].append(code_line_number)

	- namedtuple: Factory Function for Tuples with Named Fields; helps improve code readability.
		Named tuple instances do not have per-instance dictionaries, so they are lightweight and require no more memory than regular tuples.
		Tuple Subclass but with extra method and fields.
			Since it's a subclass, it can be extended to add methods.
			To add more data attributes, create new NamedTuple instead like so:
				
		Extra fields: var._fields & var._field_defaults. These list field names & their defaults.
		Extra methods: NamedTuple._make(iter) to make a namedtuple, var._asdict() returns dict of fields names & values and var._replace(**kwargs) to replace named vars as per dict given
		namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)
			- rename incorrectly named fields
			- defaults associated from right most fields
			- returns tuple subclass
			- fields defined in list or string separated by whitespace
				Point = namedtuple("Point", ['x','y'])   # namedtuple("Point", "x y")
				Point = namedtuple("Point", ['x', 'y'], defaults=(0,))  # y is default to 0
				p1 = Point(10,20)
				p2 = Point(x=11,y=22)
				p3 = Point(30)
				getattr(p3,'y')	# 0

			Recipe used to handle fields coming back from CSV/DB records:
				EmployeeRecord = namedtuple('EmployeeRecord', 'name, age, title, department, paygrade')
				import csv
				for emp in map(EmployeeRecord._make, csv.reader(open("employees.csv", "rb"))):
					print(emp.name, emp.title)

			Recipe to add methods to namedtuple subclass
				class Point(namedtuple('Point', ['x', 'y'])):
					__slots__ = ()
					@property
					def hypot(self):
						return (self.x ** 2 + self.y ** 2) ** 0.5
					def __str__(self):
						return 'Point: x=%6.3f  y=%6.3f  hypot=%6.3f' % (self.x, self.y, self.hypot)
				for p in Point(3, 4), Point(14, 5/7):
					print(p)
				Point: x= 3.000  y= 4.000  hypot= 5.000

	- OrderedDict - superior to dict in ordering but less performant in space/iteration/updates. Suitable for LRU caches where ordering is often.
		Since Py3.7 dict are ordered.
		dict.popitem() returns key,value tuple from end; while in OrderedDict.popitem(last=False) can return the first key,value tuple as well. Equivalent with dict: (k := next(iter(d)), d.pop(k))
		OrderedDict.move_to_end(key) can be done in dict as (d[k] = d.pop(k)). However, dict can move an element to front which OrderedDict can do with od.move_to_end(key, last=False).
		So if you don't need the functionality to move key to front then better off just using dict.
			od = OrderedDict([('k1','v1'),('k2','v2'),('k3','v3')])
			k,v = od.popitem(last=True)		# pop from end (k3,v3)
			k,v = od.popitem(last=False)	# pop from beginning (k1,v1)
			od.move_to_end('k2')			# k1, k3, k2 (k2 moves from middle to end)
			od.move_to_end('k2', last=False) # k2, k1, k3 (moves to beginning)

	- UserDict/UserList/UserString classes to allow customized Dict/List/String subclass development by user.
		Content of Dict/List/String is accessible in member attribute .data.		

copy: shallow/deep copy compound objects
	Deep copy can run into recursion so it maintains a memo dictionary of objects already copied.
	A class needs to implement __copy__(x) and __deepcopy__(x, memo_dictionary). 
	Stuff that can't be copied e.g. stack frame/trace, socket, file 
		from copy import deepcopy


pprint: pretty print. 
	Dicts are sorted by keys. Option to put _ in numbers, indent_depth, column_width)
	Recursive objects will print "<Recursion in object with id=..>". 
	IsReadable if string can be used to eval() and reconstruct the object.
		from pprint import pprint
		pprint(some_complex_datastruct)

Coding exercises in various languages (incl Python) with solutions: https://rosettacode.org/wiki/Category:Python 
David Beazley - https://github.com/dabeaz/blog
