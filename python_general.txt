Learn:
 - concurrency: multi-threading, multi-processing, concurrent futures
 - asyncio
 - polars
 - profiling with Scalene


Args: positional or keyword
	def fn(positional_only_args, /, positional_or_keyword, *, keyword_only):


Ternary:
	a if a < b else b


Enum: (unique) Symbolic names bound to a variable. Advantages: grouping, nice repr, type safe.
	  If 2 names have same value then second serves as alias to first -- which is returned when accessing first or second such enum member.
	  Aliases are not showin while iterating over enum.
	  To force, unique values as well then use @unique.

		from enum import Enum, auto, Flag, unique

		@unique
		class RGBColors(Enum):
			RED=1			# attribute .name=RED and .value=1
			GREEN=2
			BLUE=auto()

		RGBColors(2)		# repr:: <RGBColors.GREEN: 2>
		print(RGBColors(2)) # str:: RGBColors.GREEN

	Similar to Enum, is Flag (bitset)
		class TimeInForce(Flag):
			AON = 1		# index starts from 1 even if auto() used
			DNR = 2
			DNI = 4
			FOK = auto()
			NH = auto()
		tif = TimeInForce.AON | TimeInForce.NH		# Py3.11
		for v in tif:			# iterate over flags
			print(v)
	Access: TIF['AON'] by name; TIF(1) by value, TIF.AON.value

	Enum similar to namedtuple. E.g. Animal = Enum('Animal', 'ANT BEE CAT DOG')
	To compare Enum to integers, use IntEnums (or StrEnums or IntFlags or StrFlags).


Size of:
   import sys
   sys.getsizeof([0,1,2,3])	# bytes used by this data struct - does not count recursive data structs, only 1 level deep. Custom code to do recusrive size computation.
   				# Object has ref.count + ptr.to.its.type at least + 2ptr for garbage collector ... all of which getsizeof() captures.

   PyObject has:
   	ob_refcount
   	ob_type
   PyVarObject has:
   	ob_size		# len - actual size
   PyListObject has:
   	ob_item		# pointer
   	allocated	# how many elem "could" fit in ob_item memory - max size (returned by getsizeof)
   2 Garbage collection pointers
   So a list has 7 meta-data elements. Size of empty list = 7 * 8 = 56 bytes


__slots__ can be used to save memory taken by objects + restrict from adding more member variable (obj.NewVar).
The instance loses __dicts__.
The class retains __dicts__ and gains additional __slots__ member along with descriptors to get/set these slots members implemented in C code to access member based on memory location (instead of __dict__).
	obj.slotX = 'some value' 	# calls Class.__dict__['slotX'].__set__(obj, 'some_value'
	print(obj.slotX)		# calls Class.__dict__['slotX'].__get__(obj, Class)


Profilers are of 2 types (a) Tracing which have higher initial setup but very accurate reporting (b) Sampling
cProfile is tracing type; scalene are sampler type which also supports (threads, multiprocessing, Python vs C time, GPU, mem.trend, mem.leaks, copy time between python/native library)
Profile with cProfile module:
   $ python3 -m cProfile mycode.py    # simple/basic
   $ python3 -m cProfile -o mycode.prof mycode.py && snakeviz mycode.prof

   import cProfile
   import pstats
   with cProfile.Profile() as profiler:
       my_function()
   stats = pstats.Stats(profiler)
   stats.sort_stats(pstats.SortKey.TIME).
   # stats.print_stats()		# alternatel, pip install snakeviz
   stats.dump_stats(filename='code_profile.prof')

   $ snakeviz code_profile.prof

Profiling with scalene:
   $ scalene [options] mycode.py
   	options:
   	 --reduced-profile (only show where code runs for atleast 1%)
   	 --html --outfile profile.html  (line, fn level profile)
   	 --cpu-only	(not memory but does GPU also)



Benchmarking:
    1. set PYTHONDONTWRITEBYTECODE=1		# don't cache bytecode between test runs
    2. python -m timeit -s "from mymodule import function" "function()"		# -s part not included in benchmark time
    3. timeit.timeit("code_to_test()", setup="from __main__ import code_to_test", number=10)


Faster code tips:
   - use itertools/collections e.g. count() instead of loops with var increment
    - speed vs memory trade off (generator vs list) esp if only first elelement of list is needed then next(gen)
    - numpy/numba
    - code optimization e.g. right data struct/algorithm
   	- e.g. {}, [], () faster than dict(), list(), tuple() fn calls. Tuple faster than list
   	- unique elements of list while retaining order dict.fromkeys(mylist) but elements need to be hashable
   	- DRY vs calling too many fn (overheads).
   	- look for loop-invariant stmt (can be done out of loop), use comprehensions, correct data struct(tuple, frozenset, str, bytes, list/bytearray, set, dict)


Data containers : Class/DataClass/Dict/NamedTuple
	class Pet(typing.NamedTuple):		# Pure classes faster than dataclass (50%) & namedtuple (25%)! but more code. Classes similar to dict.
	   legs:int
	   noise:str

	@dataclass
	class Pet:
	   legs:int
	   noise:str

	Pet = namedtuple("Pet","legs noise")


Dataclasses: provides __init__, __repr__ and __eq__ by default.

    from dataclasses import dataclass, field, astuple, asdict
    import inspect

    @dataclass(frozen=True, order=True)			# order provides le/ge/lt/gt, frozen allows obj value to NOT be modified after initializations.
							    # Others: kw_args=True, match=True for structural pattern matching, slots=True
    class Comment:
        id:int
        txt:str
        replies: list[str] = field(default_factory=list)	# list or any other fn that generates initial value. Factory is invoked if user doesn't specify this field while creating object. Can set "init=False" in field() to ensure that user can not provide this arg. "repr=False" to hide field from printing with repr. "compare=False" to skip the field
        				# __post__init__(self) method can be used to set attributes that depend on other member attrib.


    def main():
        comment = Comment(1,"hoo")
        print(comment)
        # comment.id=2
        # comment.set_id(2)
        # print(comment)
        comment.x = 100	# can't do if frozen
        print(comment.x)
        number, comment = astuple(comment)		# dataclass converts to tuple/dict
        pprint(inspect.getmembers(Comment, inspect.isfunction))
    if __name__ == '__main__':
        main()


match statement (for structural pattern matching) from Py3.10.
Can match single value (int/str) or other type like list/dict and other objects
Match patterns based on given sequence i.e. order is important.
	match var:
		case 0: pass
		case 1|2: pass
		case 3 if condition: pass	#  with if condition
		case 3: pass
		case _: pass
		case other: print("unknown")


Decorator:
	Enhance/Modify behavior to existing fn. E.g. add time taken by existing fn; add logging/user behavior; authentication before fn calls.
	Address cross-cutting concerns (impacts multiple fn).
	Decorator takes a fn as input and returns modified fn as output.
	Can have multiple decorator on fn. E.g. @logging @timer on fn
	Caution - decorator impacts readability/understanding of code. e.g. when multiple decorator are used. Which one kicks in first e.g. logging before timing or after.

	def my_decorator(func, logger):		# instead of using functools.partial you can create an another outer layer of fn that takes logger as arg and returns my_decorator (see https://stackoverflow.com/questions/5929107/decorators-with-parameters)
	  @functools.wraps(func)		# fixes func.__name__ not to print "wrapper" but actual fn. Fixes fn.docstring and fn.name
	  def wrapper(*args, **kwargs):
	    logger.info(f"Started function {func.__name__}")
	    ret = func(*args, **kwargs)
	    logger.info("Ended")
	    return ret
	  return wrapper

	@my_decorator(logging)		# inc = decorator(logging)(inc)   ... this is what it means and it won't work here because my_decorator needs 2 args. Use functools.partial or wrap my_decorator in another fn lets call it decorator_generator(logger)
	def inc(x):
	  return x+1

	my_decorator_with_default_logging = functools.partial(my_decortor, logger=logging)

	@my_decorator_with_default_logging
	def dec(x):
	  return x-1

	inc(3)


Functools: Fn/Operations on callables (Fn, Class, Obj)
	@cache - memoize - store return values for args
	@cache_property - cache for methods; note: @property requires a setter fn but @cache_property allows writes
	@lru_cache(maxsize=N, typed=False) - args/kwargs must be hashable. Implemented with @cache_property but slightly slower as it needs to handle max_size limits.
		- has additional data like cache_info() to show hits/misses/currsize.
		- typically for expensive/recursive compute e.g. fibonacci or news web-page fetch
	@total_ordering - adds le/ge/lt/gt methods based on 2 methods that user needs to give i.e. eq and one of le/ge/lt/gt
	@partial - simplifies fn args/kwards by pre-building partial fn object that has some args/kwargs supplied in advance so that partial fn is called with remaining args/kwargs. Similarly, @partialmethod for methods/property.
	@reduce - apply fn to iterable with a default start value e.g. sum of series
	@wraps - calls update_wrapper that updates wrapper fn to look like wrapped function. Assigns __module__, __name__, __doc__, __annotations__, __qualname__. Updates __dict__. Used with decorators specially
	@singledispatch - transform fn into single-dispatch generic fn (based on type of first arg only). Declare the default fn with @singledispatch and then register other variants based on type(s) with @fn.register(type). To check which fn will be called for a given type, test with fn.dispatch(type) .. gives address of fn (not name!).
	@singledispatchmethod - like singledispatch on methods but applies on first non-self/non-cls arg.

Itertools: Creates iterators for efficient looping
	- non-terminating/infinite iterators
		- count(start, [step]]: count() from 0/start onwards
		- cycle(list_elements): cycle('0123456789')  ... 0123456789 0123456789 0123456789
		- repeat(element, [n)]: repeat('xyx', 20)  ...xyz xyz
	- terminating on shortest input sequence:
		- accumulate(list, [func=sum]): accumulate([1,2,3], lambda x,y: x+y) gives 1,3,6 (get running totals/products/max)
		- chain(list1, list2): chain any 2 iterables one after another
		- chain.from_iterable(iterable): chain iterable of iterables
		- compress(data_list, selector_list_bool): removes elements marked as false in selector list
		- dropwhile(predicate, sequence): chopping leading sequence. i.e. predicate applied to "leading" sequence; drops leading sequence if initially predicate is true and once false, ignores predicate and generates till end of sequence.
		- filterfalse(predicate, sequence): opposite of filter(which keeps elements where predicate is true), this keeps elements where predicate is false
		- groupby(iterable, [key]): sub-iterators groups by value of key(v). E.g. for key, group in groupby(sorted('abbaab')): print key, list(value) # gives 'a' -> ['a','a','a'] and similar for 'b'
		- isslice(sequence, [start, stop, step]): like slice/range for sequence
		- pairwise(iterable): ABCD gives overlapping pairs of AB BC CD
		- starmap(function, sequence): gives sequence of fn(*seq[0]), fn(*seq[1]). i.e. fn args come from sequence (as sub-lists) and returns sequence of results
		- takewhile(predicate, sequence): opposite of dropwhile, it gives the leading sequence while predicate is true
		- tee(iterator, n): splits one iterator in N-iterators. Allows reuse/copy of iterator.
		- zip_longest(seqA, seqB, [fillValue='-']) gives tuple having first element from seqA and second frmo seqB. Any missing values are replaced with None/fillValue.
	- Combination/Permutations
		- product(seqA, seqB, [repeat=1]) Cartesion product
		- permutations(seq, length) all combinations but no repeats e.g. AA but can have AB and BA. Note: permutations are "ordered" combinations (i.e. in permutations order matters; in combinations it does not)
		- combinations(seq, length) permutations but can't have AB == BA
		- combinations_with_replacement(seq, length) allows AA

Operator: efficient operator fn
	- le, ge, lt, gt, eq, ne
	- not, is_, is_not, truth (or bool)
	- abs(x), add, sub, mul, floordiv, truediv, index (convert non-int to int for use as index), inv/invert, mod (%), pow, matmul (@), pos (postive),
		-- in-place operators (x+=y is x= operator.iadd(x,y)). Returns updated value if inputs are immutable else updates in place.
		-- iadd/iconcat (a+=b), a&=b, a<<=b, a//=b, a@=b,
	- lshift, rshift,     neg, or_, and_, xor
	- concat, contains(a,b ... b in a), countOf, indexOf, get/set/delitem
	- call
	- attrgetter/itergetter/methodcaller:
		-- f = attrgetter('name.first', 'name.last'), then call f(b) returns (b.name.first, b.name.last).
		-- g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3]).
			getcount = itemgetter(1)
			sorted(inventory, key=getcount)
			# [('orange', 1), ('banana', 2), ('apple', 3), ('pear', 5)]
		-- m = methodcaller('name', 'arg1',arg2=10)
			m(obj) calls obj.name('arg1',arg2=10)


Collections: container datatypes
	- ChainMap: create a "view" to treat many dict/mappings as one. Internally store as references to dicts as a list (so update on any dict is reflected in ChainMap view). Can be updated/deleted but impacts only first dict/map in list.
				Can create new & separate "child" ChainMaps that append to existing ones.
				Can be used to select from options ChainMap(args, env, default).
					defaults = {'user': 'guest',...}
					parser = argparse.ArgumentParser()
					parser.add_argument('-u', '--user')
					namespace = parser.parse_args()
					command_line_args = {k: v for k, v in vars(namespace).items() if v is not None}
					combined = ChainMap(command_line_args, os.environ, defaults)
					combined['user']

		da = {'a':1,'b':2}
		db = {'a':3,'c':4}		# a:3 is not visible as a from da comes first
		dc = {'k':5}
		cm = ChainMap(da,db,dc)
		dc['x']=99		# visible in ChainMap
		cm['z']=100		# adds to da
		for key in cm:
			print(key, cm[key])

    - Counter : a dict subclass for counting hashable objects. Store {Key: Counts}. Any missing key gives count 0.
				Methods: c.most_common(5), c.total(), c.subtract(another_counter)
				Addition and subtraction combine counters by adding or subtracting the counts of corresponding elements.
				Intersection and union return the minimum and maximum of corresponding counts.
				Equality and inclusion compare corresponding counts.
				**Each operation can accept inputs with signed counts, but the output will exclude results with counts of zero or less.
				c = Counter(a=3, b=1, c=0)
				d = Counter(a=1, b=2, d=-1)
				c + d                       # add two counters together:  c[x] + d[x]
				# Counter({'a': 4, 'b': 3})
				c - d                       # subtract (keeping only positive counts)
				# Counter({'a': 2, 'd': 1})
				c & d                       # intersection:  min(c[x], d[x])
				# Counter({'a': 1, 'b': 1})
				c | d
				Counter({'a': 3, 'b': 2})
				-d (or +d) removes zero/negative counts from result
				Counter({'d': 1})  (or Counter({'b': 2, 'a': 1}))

	- deque (double ended queue). Called "deck".
			Optimized for stack(LIFO) and queue(FIFO) operations in O(1)
			Can be unbounded or length given - in which case any new adds once size limit is reached causes overflow from other end.
			Recipe to use for moving average of 3 elements.
				dq = deque('abcdef', maxlen=5)	# gives bcdef
				dq.append('g')		# cdefg
				dq.appendleft('b')	# bcdef
				f = dq.pop()		# bcde
				b = dq.popleft()	# cde
				c,e = dq[0], dq[-1]
				dq.rotate(1)		# ecd
				dq.rotate(-1)		# cde
				dq.extend('xyz')	# dexyz
				dq.extendleft('ABC') # CBAde <<note the order of insert
				dq.clear()

	- defaultdict(default_factory=None):
		Returns dict like object that has __missing__ method added (called by __getitem__) that invokes default_factory (if defined else KeyError) to setup a default value.
		Note: default_factory is not called for d.get() but only for d['item'] (__getitem__)
		Note: Py3.9 adds dict merge(|) and update(|=).

		Faster than an equivalent technique using dict.setdefault()
			for k, v in s:
    			d.setdefault(k, []).append(v)   # without defaultdict
		Example:
			exception_tracking = defaultdict(list)
			exception_tracking[exception].append(code_line_number)

	- namedtuple: Factory Function for Tuples with Named Fields; helps improve code readability.
		Named tuple instances do not have per-instance dictionaries, so they are lightweight and require no more memory than regular tuples.
		Tuple Subclass but with extra method and fields.
			Since it's a subclass, it can be extended to add methods.
			To add more data attributes, create new NamedTuple instead like so:

		Extra fields: var._fields & var._field_defaults. These list field names & their defaults.
		Extra methods: NamedTuple._make(iter) to make a namedtuple, var._asdict() returns dict of fields names & values and var._replace(**kwargs) to replace named vars as per dict given
		namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)
			- rename incorrectly named fields
			- defaults associated from right most fields
			- returns tuple subclass
			- fields defined in list or string separated by whitespace
				Point = namedtuple("Point", ['x','y'])   # namedtuple("Point", "x y")
				Point = namedtuple("Point", ['x', 'y'], defaults=(0,))  # y is default to 0
				p1 = Point(10,20)
				p2 = Point(x=11,y=22)
				p3 = Point(30)
				getattr(p3,'y')	# 0

			Recipe used to handle fields coming back from CSV/DB records:
				EmployeeRecord = namedtuple('EmployeeRecord', 'name, age, title, department, paygrade')
				import csv
				for emp in map(EmployeeRecord._make, csv.reader(open("employees.csv", "rb"))):
					print(emp.name, emp.title)

			Recipe to add methods to namedtuple subclass
				class Point(namedtuple('Point', ['x', 'y'])):
					__slots__ = ()
					@property
					def hypot(self):
						return (self.x ** 2 + self.y ** 2) ** 0.5
					def __str__(self):
						return 'Point: x=%6.3f  y=%6.3f  hypot=%6.3f' % (self.x, self.y, self.hypot)
				for p in Point(3, 4), Point(14, 5/7):
					print(p)
				Point: x= 3.000  y= 4.000  hypot= 5.000

	- OrderedDict - superior to dict in ordering but less performant in space/iteration/updates. Suitable for LRU caches where ordering is often.
		Since Py3.7 dict are ordered.
		dict.popitem() returns key,value tuple from end; while in OrderedDict.popitem(last=False) can return the first key,value tuple as well. Equivalent with dict: (k := next(iter(d)), d.pop(k))
		OrderedDict.move_to_end(key) can be done in dict as (d[k] = d.pop(k)). However, dict can move an element to front which OrderedDict can do with od.move_to_end(key, last=False).
		So if you don't need the functionality to move key to front then better off just using dict.
			od = OrderedDict([('k1','v1'),('k2','v2'),('k3','v3')])
			k,v = od.popitem(last=True)		# pop from end (k3,v3)
			k,v = od.popitem(last=False)	# pop from beginning (k1,v1)
			od.move_to_end('k2')			# k1, k3, k2 (k2 moves from middle to end)
			od.move_to_end('k2', last=False) # k2, k1, k3 (moves to beginning)

	- UserDict/UserList/UserString classes to allow customized Dict/List/String subclass development by user.
		Content of Dict/List/String is accessible in member attribute .data.


Generator: a fn that uses yield.
	Returns iterable object (once), items are generated lazily so memory efficient when handling large datasets.
	Generator fn uses "yield" instead of "return'.
	For-loop uses iter.next() until StopIteration is hit.
	for i in (x for x in range(10) if x%2==0):
		print(f"Geneterated {i}")

	Use generators to setup processing pipelines where each is fn that starts a for loop and does a yield.
		def process(sequence):
			for s in sequence:
				# something with s
				yield processed_s

	Less known is the fact that yield can also recieve values! e.g. item = yield
	Need to advance to yield stmt with next(s) first and then send data (s.send('x'))
		g = generator()							 def generator():
													try:
		g.next()		# advance to yield				item = yield   # send() sets item=item
		g.send(item)	# send data to generator		yield some_value
		g.close()       # shutting down processing	except GeneratorExit:   # close called/shutdown
		g.throw(RuntimeError,'Broken')				except RuntimeError as e: # exception thrown
													return "value"     # raises StopIteration with value=value

	Delegate to sub-generator with "yield from <gen>". Allows stacking/sequencing of generators.

	A coroutine (enter/exit & resume at many different points with send()) vs subroutine(where entry & exit path is fixed). 
	A coroutine is for consuming data; while generators are for producing data.


collections.abc - Abstract base classes for collections
	issubclass/isinstance works in one of 3 ways:
	1. inheritance from ABC. e.g. class MyClass(Sequence):...
	2. registering as virtual subclass. e.g. Sequence.register(MyClass) where MyClass does not inherit from Sequence
	3. by presense of specific dunder methods

	Avaialble ABC (with abstract methods):
		Container (__contains__)
		Hashable  (__hash__)
		Iterable  (__iter__)
		Iterator  (__next__)
		Reversible (__reversed__)
		Generator | Coroutine (send, throw)
		Sized	(__len__)
		*Callable (__call__)
		*Collection 	(Sized __len__, Iterable __iter__, Container __contains__)
		*Sequence	(Collection, Reversible __reveresed__)
			- MutableSequence (addition __get|set|delitem__)
			- ByteString
		Set (Collection) | MutableSet (Set)
		Mapping (Collection) | MutableMapping (Mapping)
		MappingView (Sized) | KeysView (MappingView, Set) | ValuesView (MappingView, Collection) | ItemsView (MappingView, Set)
		Awaitable (__await__) | AsyncIterable (__aiter__) | AsyncIterator (__anext__) | AsyncGenerator (asend|athrow)


Abstract base class (abc):
	Multiple inheritance may lead to metaclass conflicts
	1. via inheritance of ABC or ABCMeta (which has additional metho like .mro and .register(some_other_subclass) )
		from abc import ABC
		class MyABC(ABC):
			pass
	2.register the subclass (note: it won't show up in __mro__)
		MyABC.register(tuple) 	# MyABC is a subclass of tuple
		assert issubclass(tuple, MyABC)
		assert isinstance((), MyABC)

	A class that has a metaclass derived from ABCMeta cannot be instantiated unless all of its abstract methods and properties are overridden.
	class C(ABC):
		# @abstractmethod (simple, @classmethod, @staticmethod)
		@abstractmethod
		def my_abstract_method(self, arg1):
			...
		@classmethod		# used to define alternate constructors + handle inheritance issues where return by class-name is done instead of using "cls" var
		@abstractmethod
		def my_abstract_classmethod(cls, arg2):
			...
		@staticmethod		# loosely tied to class methods e.g. validate input before creating object
		@abstractmethod
		def my_abstract_staticmethod(arg3):
			...
		## property (get/set)
		@property
		@abstractmethod
		def my_abstract_property(self):
			...
		@my_abstract_property.setter
		@abstractmethod
		def my_abstract_property(self, val):
			...
		## another way to set (abstract) property
		@abstractmethod
		def _get_x(self):
			...
		@abstractmethod
		def _set_x(self, val):
			...
		x = property(_get_x, _set_x)


contextlib: for invoking tasks "with expr as target: <code block/suite>"
	Set up runtime context by implementing enter/exit fn for a code suite.
	contextmanager.__enter__() : returns this object (or related to context) and assigned to target.
	contextmanager.__exit__(exception_type, exception_value, exception_traceback) : Exit runtime context & return bool flag(true) to indiate if any exception is to be suppressed.
		Returning true causes any exception to be subpressed. Any exeption that occur in __exit__ will replace the exception raised in with block.

	@contextmanager decorator - to define factory fn that sets up a resource in __enter__, yields a resource (causing code suite to run) and frees at __exit__.
	with closing(resource) as fh:	# closign will call resource.close() in __exit__ (no setup)
		...

        # example
        def open_file(filename):
            fh = open(filename, 'r')
            try:
                yield fh
            finally:
                fh.close()

	# context manager with generator (yield) to split fn into 2 parts where each is called at __enter__ and __exit__
	@contextmanager
	def time_this(label):							with time_this("counting"):
		start = time.time()								n = 1_000_000
		try:											while n: n -=1
			yield	# cuts fn into top being called at __enter__ (imagine a next() call)
					# and bottom half is called at __exit__
		finally
			end = time.time()
			print("Time taken by {} was {:0.4f}s".format(label, end-start))

	nullcontext(enter_result) - returns enter_result on entry. Standin for optional conext manager (e.g. if/else where one part does proper content management and else part does nullcontext)
		if isinstance(file_or_path, str):        # If string, open file
			cm = open(file_or_path)
		else:        # Caller is responsible for closing file
			cm = nullcontext(file_or_path)
		with cm as file:
			# Perform processing on the file
	suppress(*Exceptions) -
		with suppress(FileNotFoundError):
			os.remove('somefile.tmp')
	redirect_stdout(new_target): Context manager for temporarily redirecting sys.stdout to another file or file-like object.
		with open('help.txt', 'w') as f:
			with redirect_stdout(f):
				help(pow)
	redirect_stderr
	chdir(path): change directory and restore when done. Not thread safe; changing global state of program
	ContextDecorator: A base class that enables a context manager to also be used as a decorator.
		Context managers inheriting from ContextDecorator have to implement __enter__ and __exit__ as normal.
		__exit__ retains its optional exception handling even when used as a decorator.
			class mycontext(ContextDecorator):
				def __enter__(self):
					print('Starting')
					return self

				def __exit__(self, *exc):
					print('Finishing')
					return False
			@mycontext()
			def function():
				print('The bit in the middle')
			# use as below, can test as function()
			with mycontext as tgt:
			    print("something")
	ExitStack : A context manager that is designed to make it easy to programmatically combine other context managers and cleanup functions,
			especially those that are optional or otherwise driven by input data.
			Similar to having nested with-statement. Maintains stack of callsbacks that are called in LIFO when instance is closed.
			with ExitStack() as stack:
    			files = [stack.enter_context(open(fname)) for fname in filenames]
	Receipe/Usecases:
		1. Handling multiple context managers
		2. Handling exception handling in __enter__ as external resource API maybe badly desgined. Note: You want to throw exceptions from with-codesuite.

	Context managers tend to be One-shot (single use) context managers.
	Re-enterant context managers example suppress(), redirect_stdout(), chdir(). threading.Rlock()
	Re-usable (but no re-enterant) context managers (example ExitStack, threading.Lock)


Descriptors:
	Allow to customize attribute lookup, storage and deletion (by defining methods __get__, __set__, __delete__ and optional __set_name__)
	Obj.attrib is first looked in dictionary (in order of object dict, class dict, classes dict in MRO, __get__ method).
	classmethod/staticmethod/property/slots/functools.cached_property/super are examples of descriptors.

	Protocol:
		descr.__get__(self, obj, type=None) -> value
		descr.__set__(self, obj, value) -> None
		descr.__delete__(self, obj) -> None

		Object defining __set__ or __delete__ is called data descriptor. With only __get__ it's a non-data descriptor.
		The difference is that in case of conflict in name between data descriptor vs name in dict, data descriptor takes precedence.
		For non-data (get) descriptor, the dict value takes precedence.
		Instance lookup scans through a chain of namespaces giving data descriptors the highest priority, followed by instance variables, then non-data descriptors, then class variables, and lastly __getattr__() if it is provided.
		a.x is really desc.__get__(a, type(a)).

		To make attribute read-only, define __get__; and for __set__, raise AttributeError


	Allows to create dynamic lookups e.g. in case below to "get" size of a directory (as files are added/deleted)
		import os
		class DirectorySize:
			def __get__(self, obj, objType=None):
				return len(os.listdir(obj.dirname))
			# can have __set__ and __del__ methods as well
			def __set__(self, obj, value):
				print(obj, value)
				obj.size = value		# hard coding of attribute (size)
		class Directory:
			size = DirectorySize()		# descriptor instance ... must be class variable (not instance variable)
			def __init__(self, dirname):
				self.dirname = dirname
		#
		g = Directory("somePath/someDir")
		g.size()		# 3			# descriptor is running a computation
		os.remove("somePath/someDir/someFile")
		g.size() 		# now 2

	Managed Attributes - to manage access to obj data/attributes.
	Descriptor assigned to an attribute e.g. DirectorySize() in class dict.
	Actual data stored in object.__dict__ as private _attribute.
	See link for a more generic example where Descriptor class is not hard-coded to one specific attribute name with __set_name__. See https://docs.python.org/3/howto/descriptor.html#customized-names

	Complete practical example for Descriptors:
	class Validator:
		"""Generic Descriptor & abstract class that requires implementation of validate()"""
		def __set_name__(self, owner, name):
			print(f"__set_name__: {owner}, {name}")
			self._owner = owner
			self.private_name = "_" + name

		def __get__(self, object, objType):
			print(f"__get__: {object}, {self.private_name}")
			return getattr(object, self.private_name)

		def __set__(self, object, value):
			self.validate(value)
			print(f"__set__: {object}, {self.private_name}, {value}")
			setattr(object, self.private_name, value)

		@abstractmethod
		def validate(self, value):
			...
#
	class OneOf(Validator):
		def __init__(self, *options) -> None:
			super().__init__()
			self._options = set(options)
		def validate(self, value):
			if not value in self._options:
				raise RuntimeError(
					f"Value {value} is not one of available options {self._options}"
				)
#
	class Number(Validator):
		def __init__(self, min=None, max=None) -> None:
			print(f"Number init {min} - {max}")
			super().__init__()
			self.min = min
			self.max = max
		def validate(self, value):
			# return super().validate(value)
			print(f"Number validate {value} with range {self.min} - {self.max}")
			if not isinstance(value, int | float):
				raise RuntimeError("Not int or float")
			if self.min and value < self.min:
				raise RuntimeError(f"Below min value of {self.min} ")
			if self.max and value > self.max:
				raise RuntimeError(f"Above max value of {self.max} ")
#
	class Test:
		num = Number(1, 10)
		big_num = Number(100, 1000)
		opt = OneOf("plastic", "wood")

		def __init__(self, num, big_num, opt) -> None:
			self.num = num
			self.big_num = big_num
			self.opt = opt
#
	t = Test(1, 400, opt="w00d")
	t.num = -100		# set throws error



Typing:

		from collections.abc import Sequence, Callable, Iterable
		from typing import NewType, TypeAlias, Protocol

		# TypeAlias simple
		Vector: TypeAlias = list[float]  # type alias

		# TypeAlias (complex)
		ConnectionOptions: TypeAlias = dict[str, str]  # user: password
		Address: TypeAlias = tuple[str, int]  # host:port
		Server: TypeAlias = tuple[Address, ConnectionOptions]


		def scale(scalar: float, vector: Vector) -> Vector:
		    return [scalar * x for x in vector]


		def broadcast_msg(msg: str, servers: Sequence[Server]) -> None:  # Sequence
		    for server in servers:
			print(server, msg)


		# NewType
		UserId = NewType("UserId", int)  # UserId subclass of int
		SuperUserId = NewType("SuperUserId", UserId)
		student_id = UserId(123)
		if 123 == UserId(123):
		    print("yes")

		Base: TypeAlias = int
		Derived = NewType("Derived", Base)


		# Callable objects
		def feeder(get_next_item: Callable[[int], None]) -> None:
		    # Callables takes one arg (int) and returns None
		    # Callable with multiple (3) args may look like Callable[[int,str,UserId], UserId]
		    # use ... for arbitary arg list
		    # For keyword args, variable args, or overloaded fn; define using Protocol and __call__() ???
		    pass


io - for working with streams (text/binary/raw generic; file object concrete)
	Text IO works with str (binary with byes).
	Text steam : f = open("somefile", "r", encoding="utf-8")
	In-memory text stream : f = io.StringIO("some data is in here")
	
	Binary IO: file open in "rb" mode and in-memory with io.BytesIO

array - compact representation of numeric/char array (same type restriction vs list).
	Same method as list except itemsize + typecode. Init from list/iterable/bytes

	ar = array.array('i', [1,2,3,4,5])

	element_size = ar.itemsize	# 4
	datatype = ar.typecode 		# i

	array.typecodes			# 'bBhHiIlLqQfd'


copy: shallow/deep copy compound objects
	Deep copy can run into recursion so it maintains a memo dictionary of objects already copied.
	A class needs to implement __copy__(x) and __deepcopy__(x, memo_dictionary).
	Stuff that can't be copied e.g. stack frame/trace, socket, file
		from copy import deepcopy
		x_copy =  deepcopy(x)


pprint: pretty print.
	Dicts are sorted by keys. Option to put _ in numbers, indent_depth, column_width)
	Recursive objects will print "<Recursion in object with id=..>".
	IsReadable if string can be used to eval() and reconstruct the object.
		from pprint import pprint
		pprint(some_complex_datastruct)


pickle:
        binary serialization/unserialization protocol for python objects (need to share/import class definition).
        Great if both consumer/producer are python based.
        Pickle is backward compatible by specifying pickling protocol(1-5).
        Pickle data can be further compressed with zip/lzm4 etc
        Caution: never unserialize untrusted pickled data.

        Does not work with and raises PickingError exception:
        1. Lambda  (relies on being able to pickle fn. name only)
        2. Classes/fn not at top level of module (i.e nested - only instance data is pickled; not class code/data so such classes/fn should be importable when unpickling)
        3. Deeply recursive data structs (> sys.setrecursionlimit())

        dumps/loads work with bytes.
        dump/load work with fh.write or io.BytesIO

		For large data transfer/pickling - out-of-band buffers were added in Protocol v5 and above. This allows provider/consumer to handle PickleBuffers instead of just bytes. Requires provider to implement __reduce__ex__() & consumer provides a buffer for unpickling apart from unpickled byte-stream.


JSON:
        text format; human readable; inter-operable outside python.
        No support for custom clases; no vulnerability while deserializing JSON.


f-string (f"...{var}..."
        f"{var=}"	# var=10
        f"{var%2=}"	# var%2=0
        f"{var=:FORMAT_STRING}"	# format string is related to type. Invokes __format__ method of class/type
        f"{now=:%y-%m-%d}"
        f"{float_number=:.2f}"
        f"{var!a}"	# !a -> ascii (converts non-ascii e.g. unicode into \U)
        f"{var!r}"	# !r -> print repr equivalent
        f"{var!s}"	# !s -> print str (default)


string:
        string.ascii_letters | ascii_lowercase | ascii_uppercase
        string.digits | hexdigits | octdigits
        string.punctuation
        string.whitespace
        string.printable (letters + digits + punctuation + whitespace)
        string.capwords(s, sep=None)  # capitalize each word (unlike str.capitalize which does first char only) .. also removes lead/trail/multiple whitespaces.


        conversion flags: "More {var!r}"   # '!s' str(), '!r' repr() and '!a' ascii().
        # format ':...' is [[fill]align][sign]["z"]["#"]["0"][width][grouping_option]["." precision][type]

        alignment: '{:<10}'.format('left')      # default for str
                   '{:>10}'.format('right')     # default for numbers
                   '{:*^10}'.format('center *fill rest')   # '{:{fill}{align}WIDTH}'
        sign: '{:+f} {: f} {:-f}'.format(no1, no2, no3)
              # show +/- sign; space if +; only - for last (default).
        dec/oct/hex: '{:0d} {:0o} {:0x} {:0e} {:.2f} {:0c}'.format(...)  # dec, oct, hex, float, scientific, char
        separator : '{:,} {:_}'.format(bigno1, bigno2) # use , or _ separator
        percentage: '{:.2f%}'   # 2 decimal places with % at end
        datetime: : '{:%Y-%m-%d %H:%M:%S}'.format(d)

        Template strings: Does $var substitution. Also has safe_substitute in case a var is not supplied (in which case it displays '$var' as-is.
        from strings import Template
        template_str = '$name is $age years old'
        template_str.substitute(name='Adi', age=44)     # Adi is 44 years old
        template_str.safe_substitute(name='Adi')        # Adi is $age years old


re: for regular expression
        import re
        # pattern r'..' doesn't interpret special charcters so no need to escape without which escaping \ as \\ in pattern needs to be written as \\\\. Better off with r'\\'
        # both pattern & text need to be str or bytes
        match = re.search(pattern, text)
        if match:
                match.group()


        # special
        \d (digit)
        \w (word char that covers digits)
        \s (whitespace)  \S non-whitespace
        Greedy vs Non-greedy matches :: <.*> vs <.*?>
        Range : a{2,6}  aa to aaaaaa. Non-greedy a{2,6}? will match only aa if aaa to aaaaaa.
        Possesive Quantifiers (new v3.11): *+, ++, ?+ and {m,n}+
                unlike their greedy counterparts, they don't allow backtracking
                so a*a will match a* with all string 'aaaa' and final a will fail to match and regex match fail. Same as (?>a*)
        (...) remember the matched string and access via \1 in rest of pattern
        (?P<name>...) & (?P=<name>)  instead of using \1 use python variable name <name> and reuse with P= in pattern. e.g. (?P<quote>['"]).*?(?P=quote)
        (?=...) or (?!...) : matches if ... matches next. e.g. 'Isaac (?=Asimov)' will pass only for 'Isaac Asimov' but not for 'Isaac A'
        (?<=...) Positive Lookbehind Assertion. .e.g (?<=abc)def will match abcdef as it will match deg and then lookback to ensure last 3 char are abc.
        (?iMaS...) match pattern but ignore-case multi-line ascii dot-all(S)
        (?#comment)

        \A \Z - start/end of string
        \b - boundary between \w and \W (word boundary)
        \d \w \s (\w is alhpanum; \s is whitespace)

        re.X or re.VERBOSE allows writing patterns with comments and improve readability.
        # b = re.compile(r"\d+\.\d*")
        a = re.compile(r"""\d +  # the integral part
                           \.    # the decimal point
                           \d *  # some fractional digits""",
                           re.X)

        re.compile saves the pattern that can be used in match or search
        result = re.match(pattern, text)
        vs
        pattern_compiled = re.compile(pattern, flags)
        result = pattern_compiled.search(text)
        if result:
                results.

        re.match - match from beginning for text (faster but pattern needs to match from first letter)
        *re.search* - more general but slower than match. Both search & match have start & end pos args to limit the search to specific sub-string
        re.fullmatch - pattern must match entire string
        re.split(pattern, text, maxsplits=0)
        re.findall or re.finditer - return list/iter of Match objects
        re.sub(patter, replace, text, count=0, flags=0)  # replace can be str (with \1) or fn(match_obj)
        re.escape(text) - escape special characters

        match object:
        match.expand("a\g<1>c")    # abc where b was group1

        match.group() or match.group(0) gives whole string matched
        match.group(1) or match.group([1,2]) gives the groups matched as \1 \2
        match.lastindex # gives the max group so you know how many group(N) to search for

        # groups
        match.groups()   # gives all groups as a tuple of string
        match.groupdict()       # for group-named based with (?P<name>)
        match.start() | .end() | .span() returns the indicies of match in input text.
        # group-names
        match = re.match(r"(?P<first_name>\w+) (?P<last_name>\w+)", "Malcolm Reynolds")
        match.group('first_name')



Packaging options: PyPI, setup.py, wheel
	Code that uses only std.library can be distributed as .tar.gz (called "sdist" package for Source Distribution Package). Typically, adds requirement.txt for other packages needed.

	If any C/C++ library is needed then use Wheel to package these binary dependencies with code.

	When making modules, you oraganize your code in tree directroy structure where each directory has a __init__.py that can gie directions on what to import automatically.
	E.g. 	import mymodule.submodule.somepycode
	invokes mymodule/__init__.py then mymodule/submodule/__init__.py then mymodule/submodule/somepycode


Pyre : static type-checker from Facebook; pysa for security focussed tool
    $ pyre init         # sets up .pyre_configuration & .watchmanconfog
    


VSCode settings:
 - install Python from market place (also installs pylint and pylance)
 - python.analysis.typeCheckingMode: off/basic/strict   *If you follow typing hints*
 - editor.formateOnSave + editor.codeActionsOnSave (source.organiseImports=true)

Coding exercises in various languages (incl Python) with solutions: 
    https://rosettacode.org/wiki/Category:Python
David Beazley - https://github.com/dabeaz/blog
